{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91db2f3d-d7b7-47d6-98ef-08557a6d683a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "RANDOM_SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "795bb906-2e5b-4ac4-a787-27e9b166177c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A =\n",
      "[[ 0.12573022 -0.13210486  0.64042265  0.10490012 -0.53566937]\n",
      " [ 0.36159505  1.30400005  0.94708096 -0.70373524 -1.26542147]\n",
      " [-0.62327446  0.04132598 -2.32503077 -0.21879166 -1.24591095]\n",
      " [-0.73226735 -0.54425898 -0.31630016  0.41163054  1.04251337]\n",
      " [-0.12853466  1.36646347 -0.66519467  0.35151007  0.90347018]]\n",
      "\n",
      "deviation = 0.0\n"
     ]
    }
   ],
   "source": [
    "# 6.1.1: Calculating trace\n",
    "RNG = np.random.default_rng(seed=RANDOM_SEED)\n",
    "A = RNG.standard_normal(size=(5,5))\n",
    "print(\"A =\")\n",
    "print(A)\n",
    "print()\n",
    "trace = np.einsum(\"ii->\", A)\n",
    "print(\"deviation =\", np.linalg.norm(trace - np.trace(A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d0dc40e-9c84-4c93-bb76-a4a82c633bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A =\n",
      "[[ 0.12573022 -0.13210486  0.64042265  0.10490012 -0.53566937]\n",
      " [ 0.36159505  1.30400005  0.94708096 -0.70373524 -1.26542147]\n",
      " [-0.62327446  0.04132598 -2.32503077 -0.21879166 -1.24591095]\n",
      " [-0.73226735 -0.54425898 -0.31630016  0.41163054  1.04251337]\n",
      " [-0.12853466  1.36646347 -0.66519467  0.35151007  0.90347018]]\n",
      "\n",
      "B =\n",
      "[[ 0.0940123  -0.74349925 -0.92172538 -0.45772583  0.22019512]\n",
      " [-1.00961818 -0.20917557 -0.15922501  0.54084558  0.21465912]\n",
      " [ 0.35537271 -0.65382861 -0.12961363  0.78397547  1.49343115]\n",
      " [-1.25906553  1.51392377  1.34587542  0.7813114   0.26445563]\n",
      " [-0.31392281  1.45802068  1.96025832  1.80163487  1.31510376]]\n",
      "\n",
      "deviation = 1.2729041056627091e-15\n"
     ]
    }
   ],
   "source": [
    "# 6.1.2: Calculating matrix product\n",
    "RNG = np.random.default_rng(seed=RANDOM_SEED)\n",
    "A = RNG.standard_normal(size=(5,5))\n",
    "print(\"A =\")\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "B = RNG.standard_normal(size=(5,5))\n",
    "print(\"B =\")\n",
    "print(B)\n",
    "print()\n",
    "\n",
    "matmul = np.einsum(\"ik,kj->ij\", A, B)\n",
    "print(\"deviation =\", np.linalg.norm(matmul - A @ B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88a7e6f9-b668-4e48-9083-98923be92783",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A =\n",
      "[[[ 0.12573022 -0.13210486  0.64042265  0.10490012 -0.53566937]\n",
      "  [ 0.36159505  1.30400005  0.94708096 -0.70373524 -1.26542147]\n",
      "  [-0.62327446  0.04132598 -2.32503077 -0.21879166 -1.24591095]\n",
      "  [-0.73226735 -0.54425898 -0.31630016  0.41163054  1.04251337]]\n",
      "\n",
      " [[-0.12853466  1.36646347 -0.66519467  0.35151007  0.90347018]\n",
      "  [ 0.0940123  -0.74349925 -0.92172538 -0.45772583  0.22019512]\n",
      "  [-1.00961818 -0.20917557 -0.15922501  0.54084558  0.21465912]\n",
      "  [ 0.35537271 -0.65382861 -0.12961363  0.78397547  1.49343115]]\n",
      "\n",
      " [[-1.25906553  1.51392377  1.34587542  0.7813114   0.26445563]\n",
      "  [-0.31392281  1.45802068  1.96025832  1.80163487  1.31510376]\n",
      "  [ 0.35738041 -1.20831863 -0.00445413  0.65647494 -1.28836146]\n",
      "  [ 0.39512206  0.42986369  0.69604272 -1.18411797 -0.66170257]]]\n",
      "\n",
      "B =\n",
      "[[[-0.43643525 -1.16980191  1.73936788 -0.49591073  0.32896963\n",
      "   -0.25857255]\n",
      "  [ 1.58347288  1.32036099  0.63335262 -2.20350988  0.05202897\n",
      "    0.68368619]\n",
      "  [ 1.00396158 -0.61790704  1.82201136 -1.32043097 -0.66152802\n",
      "    0.93504999]\n",
      "  [ 0.04905461  2.00239258  0.18851919 -0.63319409 -0.37756351\n",
      "   -1.09114612]\n",
      "  [-1.27768017  0.63041149  0.58116581  1.29455882 -0.75460579\n",
      "    1.68910745]]\n",
      "\n",
      " [[-0.28738771  1.57440828 -0.43278585 -0.73548329  0.24978537\n",
      "    1.03145308]\n",
      "  [ 0.16100958 -0.58552882 -1.34121971 -1.40152021  0.50268285\n",
      "    0.98971303]\n",
      "  [-0.16429459 -1.07436486  0.87304215 -1.28039394 -0.7130681\n",
      "    0.62101785]\n",
      "  [-2.25014117  0.3863696  -0.58164084  0.1092797  -0.07570153\n",
      "    0.2021144 ]\n",
      "  [ 0.69417194 -0.75836975  1.42098202  0.72609379  0.84373266\n",
      "    1.16486398]]\n",
      "\n",
      " [[ 0.78758822  0.84407868  0.07559361 -1.42677385 -0.1350451\n",
      "   -0.76951464]\n",
      "  [-1.42274177  0.25845279 -0.56854945 -1.02980444 -1.04300108\n",
      "    0.26841708]\n",
      "  [ 0.35867195  1.32245747 -0.01391467  1.04183976  1.40226483\n",
      "    1.15016564]\n",
      "  [-2.36530391  1.22868372  0.33962001  0.42377135  0.37122742\n",
      "    0.38275716]\n",
      "  [ 0.31941422 -0.35891331 -1.9016353  -0.10891473 -0.80373185\n",
      "    1.08016341]]]\n",
      "\n",
      "deviation = 1.550563343800555e-15\n"
     ]
    }
   ],
   "source": [
    "# 6.1.3: Calculating batchwise product\n",
    "RNG = np.random.default_rng(seed=RANDOM_SEED)\n",
    "A = RNG.standard_normal(size=(3, 4, 5))\n",
    "print(\"A =\")\n",
    "print(A)\n",
    "print()\n",
    "\n",
    "B = RNG.standard_normal(size=(3, 5, 6))\n",
    "print(\"B =\")\n",
    "print(B)\n",
    "print()\n",
    "\n",
    "batchmul = np.einsum(\"bik,bkj->bij\", A, B)\n",
    "manual_batchmul = []\n",
    "for b in range(A.shape[0]):\n",
    "    manual_batchmul.append(A[b] @ B[b])\n",
    "manual_batchmul = np.array(manual_batchmul)\n",
    "print(\"deviation =\", np.linalg.norm(batchmul - manual_batchmul))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef4b4fa2-cfaf-4da0-9b42-2b141562f2a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24804, 25155, 25506, 25857],\n",
       "       [62712, 63792, 64872, 65952]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.arange(2 * 3 * 3 * 3).reshape((2, 3, 3, 3))\n",
    "t2 = np.arange(3 * 3 * 3 * 4).reshape((3, 3, 3, 4))\n",
    "np.einsum(\"bijc,ijco->bo\", t, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d63fe5fb-ad72-4881-8237-1efdc6c8ce7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27., 27., 27., 27.],\n",
       "       [27., 27., 27., 27.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.ones(2 * 3 * 3 * 3).reshape((2, 3, 3, 3))\n",
    "t2 = np.ones(3 * 3 * 3 * 4).reshape((3, 3, 3, 4))\n",
    "np.einsum(\"bijc,ijco->bo\", t, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28ee7520-c32e-4a59-9752-b0d9891442c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 3)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.arange(2 * 3 * 3 * 3).reshape((2, 3, 3, 3))\n",
    "np.argmax(t, axis = 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c5e99-7c8b-432d-a7b0-0c492b87f3c6",
   "metadata": {},
   "source": [
    "## Activation Function Implementations:\n",
    "\n",
    "Implementation of `activations.Linear`:\n",
    "\n",
    "```python\n",
    "class Linear(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for f(z) = z.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        return Z\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for f(z) = z.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  gradient of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        return dY\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `activations.Sigmoid`:\n",
    "\n",
    "```python\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for sigmoid function:\n",
    "        f(z) = 1 / (1 + exp(-z))\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return np.max()\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for sigmoid.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  gradient of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return ...\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `activations.ReLU`:\n",
    "\n",
    "```python\n",
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for relu activation:\n",
    "        f(z) = z if z >= 0\n",
    "               0 otherwise\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return np.maximum(Z, 0)\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for relu activation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  gradient of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return dY * (Z > 0)\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `activations.SoftMax`:\n",
    "\n",
    "```python\n",
    "class SoftMax(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, Z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for softmax activation.\n",
    "        Hint: The naive implementation might not be numerically stable.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z  input pre-activations (any shape)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f(z) as described above applied elementwise to `Z`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        m = np.max(Z, axis=-1, keepdims=True) # axis=-1 -> apply max over last dim -> each class gets its own stabilization\n",
    "        stable = Z - m\n",
    "        exponentiated = np.exp(stable)\n",
    "        distribution = np.divide(exponentiated, np.sum(exponentiated, axis=-1, keepdims=True))\n",
    "        return distribution\n",
    "\n",
    "    def backward(self, Z: np.ndarray, dY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for softmax activation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z   input to `forward` method\n",
    "        dY  gradient of loss w.r.t. the output of this layer\n",
    "            same shape as `Z`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss w.r.t. input of this layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        batch_sigmas = self.forward(Z)\n",
    "        grads = [] # could fix with einsum but its good enough for now\n",
    "        for sample_idx in range(batch_sigmas.shape[0]):\n",
    "            sample_sigmas = batch_sigmas[sample_idx]\n",
    "            diag_sigmas = np.diagflat(sample_sigmas) # diag(sigma_i)\n",
    "            sample_sigmas = sample_sigmas.reshape((-1, 1)) # make a column vector to allow linalg\n",
    "            dSigma = diag_sigmas - (sample_sigmas @ sample_sigmas.T) # diag(sigma_i) - [sigma_i * sigma_j] for all i,j <= k \n",
    "            # -> [s_i(1 - si) on diags, 0 - s_i*s_j off diag] :)\n",
    "            grads.append(dY[sample_idx] @ dSigma)\n",
    "        return np.array(grads)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Layer Implementations:\n",
    "\n",
    "Implementation of `layers.FullyConnected`:\n",
    "\n",
    "```python\n",
    "class FullyConnected(Layer):\n",
    "    \"\"\"A fully-connected layer multiplies its input by a weight matrix, adds\n",
    "    a bias, and then applies an activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_out: int, activation: str, weight_init=\"xavier_uniform\"\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_in = None\n",
    "        self.n_out = n_out\n",
    "        self.activation = initialize_activation(activation)\n",
    "\n",
    "        # instantiate the weight initializer\n",
    "        self.init_weights = initialize_weights(weight_init, activation=activation)\n",
    "\n",
    "    def _init_parameters(self, X_shape: Tuple[int, int]) -> None:\n",
    "        \"\"\"Initialize all layer parameters (weights, biases).\"\"\"\n",
    "        self.n_in = X_shape[1]\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        W = self.init_weights((self.n_in, self.n_out))\n",
    "        b = np.zeros((1, self.n_out))\n",
    "\n",
    "        self.parameters = OrderedDict({\"W\": W, \"b\": b}) # DO NOT CHANGE THE KEYS\n",
    "        self.cache = OrderedDict({\"Z\": [],\"X\": []})  # cache for backprop\n",
    "        self.gradients = OrderedDict({\"W\": np.zeros_like(W), \"b\": np.zeros_like(b)}) # parameter gradients initialized to zero  \n",
    "                                                                                     # MUST HAVE THE SAME KEYS AS `self.parameters`\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass: multiply by a weight matrix, add a bias, apply activation.\n",
    "        Also, store all necessary intermediate results in the `cache` dictionary\n",
    "        to be able to compute the backward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input matrix of shape (batch_size, input_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a matrix of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # initialize layer parameters if they have not been initialized\n",
    "        if self.n_in is None:\n",
    "            self._init_parameters(X.shape)\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        \n",
    "        # perform an affine transformation and activation\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "        Z = X @ W + b\n",
    "        out = self.activation(Z)\n",
    "        \n",
    "        # store information necessary for backprop in `self.cache`\n",
    "        self.cache[\"Z\"] = Z\n",
    "        self.cache[\"X\"] = X\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for fully connected layer.\n",
    "        Compute the gradients of the loss with respect to:\n",
    "            1. the weights of this layer (mutate the `gradients` dictionary)\n",
    "            2. the bias of this layer (mutate the `gradients` dictionary)\n",
    "            3. the input of this layer (return this)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  gradient of the loss with respect to the output of this layer\n",
    "              shape (batch_size, output_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of the loss with respect to the input of this layer\n",
    "        shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "        \n",
    "        # unpack the cache\n",
    "        W = self.parameters[\"W\"]\n",
    "        # b = self.parameters[\"b\"]\n",
    "        Z = self.cache[\"Z\"]\n",
    "        X = self.cache[\"X\"]\n",
    "\n",
    "        # compute the gradients of the loss w.r.t. all parameters as well as the\n",
    "        # input of the layer\n",
    "        dZ = self.activation.backward(Z, dLdY)\n",
    "        dW = X.T @ dZ\n",
    "        db = np.sum(dZ, axis=0, keepdims=True) # keepdims -> shape = (1, n^[l + 1]) as opposed to (n^[l + 1])\n",
    "        dX = dZ @ W.T\n",
    "\n",
    "        # store the gradients in `self.gradients`\n",
    "        # the gradient for self.parameters[\"W\"] should be stored in\n",
    "        # self.gradients[\"W\"], etc.\n",
    "        self.gradients[\"W\"] = dW\n",
    "        self.gradients[\"b\"] = db\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return dX\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `layers.Pool2D`:\n",
    "\n",
    "```python\n",
    "class Pool2D(Layer):\n",
    "    \"\"\"Pooling layer, implements max and average pooling.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_shape: Tuple[int, int],\n",
    "        mode: str = \"max\",\n",
    "        stride: int = 1,\n",
    "        pad: Union[int, Literal[\"same\"], Literal[\"valid\"]] = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        if type(kernel_shape) == int:\n",
    "            kernel_shape = (kernel_shape, kernel_shape)\n",
    "\n",
    "        self.kernel_shape = kernel_shape\n",
    "        self.stride = stride\n",
    "\n",
    "        if pad == \"same\":\n",
    "            self.pad = ((kernel_shape[0] - 1) // 2, (kernel_shape[1] - 1) // 2)\n",
    "        elif pad == \"valid\":\n",
    "            self.pad = (0, 0)\n",
    "        elif isinstance(pad, int):\n",
    "            self.pad = (pad, pad)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Pad mode found in self.pad.\")\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == \"max\":\n",
    "            self.pool_fn = np.max\n",
    "            self.arg_pool_fn = np.argmax\n",
    "        elif mode == \"average\":\n",
    "            self.pool_fn = np.mean\n",
    "\n",
    "        self.cache = {\n",
    "            \"out_rows\": [],\n",
    "            \"out_cols\": [],\n",
    "            \"X_pad\": [],\n",
    "            \"p\": [],\n",
    "            \"pool_shape\": [],\n",
    "        }\n",
    "        self.parameters = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass: use the pooling function to aggregate local information\n",
    "        in the input. This layer typically reduces the spatial dimensionality of\n",
    "        the input while keeping the number of feature maps the same.\n",
    "\n",
    "        As with all other layers, please make sure to cache the appropriate\n",
    "        information for the backward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input array of shape (batch_size, in_rows, in_cols, channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pooled array of shape (batch_size, out_rows, out_cols, channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # implement the forward pass\n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "        kernel_height, kernel_width = self.kernel_shape\n",
    "        out_rows = int((in_rows + 2 * self.pad[0] - kernel_height) / self.stride) + 1\n",
    "        out_cols = int((in_cols + 2 * self.pad[1] - kernel_width) / self.stride) + 1\n",
    "\n",
    "        X_pad = np.pad(X, ((0,), (self.pad[0],), (self.pad[1],), (0,)))\n",
    "        X_pool = np.zeros((n_examples, out_rows, out_cols, in_channels))\n",
    "\n",
    "        for row in range(out_rows):\n",
    "            y = row * self.stride\n",
    "            for col in range(out_cols):\n",
    "                x = col * self.stride\n",
    "                X_pool[:, row, col, :] = self.pool_fn(X_pad[:, y:y + kernel_height, x:x + kernel_width, :],\n",
    "                                                       axis = (1, 2))\n",
    "\n",
    "        # cache any values required for backprop\n",
    "        self.cache[\"out_rows\"] = out_rows\n",
    "        self.cache[\"out_cols\"] = out_cols\n",
    "        self.cache[\"X_pad\"] = X_pad\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return X_pool\n",
    "\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for pooling layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  gradient of loss with respect to the output of this layer\n",
    "              shape (batch_size, out_rows, out_cols, channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of loss with respect to the input of this layer\n",
    "        shape (batch_size, in_rows, in_cols, channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # perform a backward pass\n",
    "        \n",
    "        # unpack the cache\n",
    "        out_rows = self.cache[\"out_rows\"]\n",
    "        out_cols = self.cache[\"out_cols\"]\n",
    "        X_pad = self.cache[\"X_pad\"]\n",
    "\n",
    "        h, w = self.kernel_shape\n",
    "        n_examples, in_rows, in_cols, in_channels = X_pad.shape\n",
    "        in_rows = in_rows - 2 * self.pad[0]\n",
    "        in_cols = in_cols - 2 * self.pad[1]\n",
    "\n",
    "        # compute the gradients of the loss w.r.t. all parameters as well as the\n",
    "        # input of the layer\n",
    "        dX_pad = np.zeros_like(X_pad)\n",
    "\n",
    "        for row in range(out_rows):\n",
    "            y = row * self.stride\n",
    "            for col in range(out_cols):\n",
    "                x = col * self.stride\n",
    "                if self.mode == \"max\":\n",
    "                    flattened_pad = X_pad[:, y:y + h, x:x + w, :].reshape((n_examples, h * w, in_channels))\n",
    "                    kernel_idxs = self.arg_pool_fn(flattened_pad, axis = 1)\n",
    "                    batch_idxs, channel_idxs = np.indices((n_examples, in_channels))\n",
    "                    mask = np.zeros_like(flattened_pad)\n",
    "                    mask[batch_idxs, kernel_idxs, channel_idxs] = 1\n",
    "                    mask = mask.reshape((n_examples, h, w, in_channels))\n",
    "                    dX_pad[:, y:y + h, x:x + w, :] += mask * dLdY[:, row:row + 1, col:col + 1, :]\n",
    "                elif self.mode == \"average\":\n",
    "                    dX_pad[:, y:y + h, x:x + w, :] += dLdY[:, row:row + 1, col:col + 1, :] / (h * w)\n",
    "        \n",
    "        dX = dX_pad[:, self.pad[0]:self.pad[0] + in_rows, self.pad[1]:self.pad[1] + in_cols, :]\n",
    "\n",
    "        # store the gradients in `self.gradients`\n",
    "        # the gradient for self.parameters[\"W\"] should be stored in\n",
    "        # self.gradients[\"W\"], etc.\n",
    "        \n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return dX\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `layers.Conv2D.__init__`:\n",
    "\n",
    "```python\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_out: int,\n",
    "        kernel_shape: Tuple[int, int],\n",
    "        activation: str,\n",
    "        stride: int = 1,\n",
    "        pad: str = \"same\",\n",
    "        weight_init: str = \"xavier_uniform\",\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_in = None\n",
    "        self.n_out = n_out\n",
    "        self.kernel_shape = kernel_shape\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.activation = initialize_activation(activation)\n",
    "        self.init_weights = initialize_weights(weight_init, activation=activation)\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `layers.Conv2D._init_parameters`:\n",
    "\n",
    "```python\n",
    "    def _init_parameters(self, X_shape: Tuple[int, int, int, int]) -> None:\n",
    "        \"\"\"Initialize all layer parameters and determine padding.\"\"\"\n",
    "        self.n_in = X_shape[3]\n",
    "\n",
    "        W_shape = self.kernel_shape + (self.n_in,) + (self.n_out,)\n",
    "        W = self.init_weights(W_shape)\n",
    "        b = np.zeros((1, self.n_out))\n",
    "\n",
    "        self.parameters = OrderedDict({\"W\": W, \"b\": b}) # DO NOT CHANGE THE KEYS\n",
    "        self.cache = OrderedDict({\"Z\": [], \"X\": []}) # cache for backprop\n",
    "        self.gradients = OrderedDict({\"W\": np.zeros_like(W), \"b\": np.zeros_like(b)}) # parameter gradients initialized to zero\n",
    "                                                                                     # MUST HAVE THE SAME KEYS AS `self.parameters`\n",
    "\n",
    "        if self.pad == \"same\":\n",
    "            self.pad = ((W_shape[0] - 1) // 2, (W_shape[1] - 1) // 2)\n",
    "        elif self.pad == \"valid\":\n",
    "            self.pad = (0, 0)\n",
    "        elif isinstance(self.pad, int):\n",
    "            self.pad = (self.pad, self.pad)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Pad mode found in self.pad.\")\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `layers.Conv2D.forward`:\n",
    "\n",
    "```python\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass for convolutional layer. This layer convolves the input\n",
    "        `X` with a filter of weights, adds a bias term, and applies an activation\n",
    "        function to compute the output. This layer also supports padding and\n",
    "        integer strides. Intermediates necessary for the backward pass are stored\n",
    "        in the cache.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input with shape (batch_size, in_rows, in_cols, in_channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output feature maps with shape (batch_size, out_rows, out_cols, out_channels)\n",
    "        \"\"\"\n",
    "        if self.n_in is None:\n",
    "            self._init_parameters(X.shape)\n",
    "\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "\n",
    "        kernel_height, kernel_width, in_channels, out_channels = W.shape\n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "        kernel_shape = (kernel_height, kernel_width)\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # implement a convolutional forward pass\n",
    "        X_pad = np.pad(X, ((0,), (self.pad[0],), (self.pad[1],), (0,)))\n",
    "\n",
    "        out_rows = int((in_rows + 2 * self.pad[0] - kernel_height) / self.stride) + 1\n",
    "        out_cols = int((in_cols + 2 * self.pad[1] - kernel_width) / self.stride) + 1\n",
    "\n",
    "        Z = np.zeros((n_examples, out_rows, out_cols, out_channels))\n",
    "\n",
    "        for row in range(out_rows):\n",
    "            y = row * self.stride\n",
    "            for col in range(out_cols):\n",
    "                x = col * self.stride\n",
    "                Z[:, row, col, :] = np.einsum(\"bijc,ijco->bo\", \n",
    "                                              X_pad[:, y:y + kernel_height, x:x + kernel_width, :],\n",
    "                                              W) + b\n",
    "\n",
    "        out = self.activation(Z)\n",
    "\n",
    "        # cache any values required for backprop\n",
    "        self.cache[\"X\"] = X \n",
    "        self.cache[\"Z\"] = Z\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return out\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `layers.Conv2D.backward`:\n",
    "\n",
    "```python\n",
    "    def backward(self, dLdY: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass for conv layer. Computes the gradients of the output\n",
    "        with respect to the input feature maps as well as the filter weights and\n",
    "        biases.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dLdY  gradient of loss with respect to output of this layer\n",
    "              shape (batch_size, out_rows, out_cols, out_channels)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gradient of the loss with respect to the input of this layer\n",
    "        shape (batch_size, in_rows, in_cols, in_channels)\n",
    "        \"\"\"\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        # perform a backward pass\n",
    "\n",
    "        # unpack the cache\n",
    "        W = self.parameters[\"W\"]\n",
    "        # b = self.parameters[\"b\"]\n",
    "        Z = self.cache[\"Z\"]\n",
    "        X = self.cache[\"X\"]\n",
    "        X_pad = np.pad(X, ((0,), (self.pad[0],), (self.pad[1],), (0,)))\n",
    "\n",
    "        kernel_height, kernel_width, in_channels, out_channels = W.shape\n",
    "        n_examples, in_rows, in_cols, in_channels = X.shape\n",
    "\n",
    "        out_rows = int((in_rows + 2 * self.pad[0] - kernel_height) / self.stride) + 1\n",
    "        out_cols = int((in_cols + 2 * self.pad[1] - kernel_width) / self.stride) + 1\n",
    "\n",
    "        # compute the gradients of the loss w.r.t. all parameters as well as the\n",
    "        # input of the layer\n",
    "        dZ = self.activation.backward(Z, dLdY)\n",
    "        db = np.sum(dZ, axis = (0, 1, 2)).reshape(1, -1)\n",
    "        dW = np.zeros_like(W)\n",
    "        dX_pad = np.zeros_like(X_pad)\n",
    "\n",
    "        for row in range(out_rows):\n",
    "            y = row * self.stride\n",
    "            for col in range(out_cols):\n",
    "                x = col * self.stride\n",
    "                dW += np.einsum(\"bijo,bklc->klco\",\n",
    "                                dZ[:, row:row+1, col:col+1, :], \n",
    "                                X_pad[:,y:y + kernel_height, x:x + kernel_width,:])\n",
    "                dX_pad[:, y:y + kernel_height, x:x + kernel_width, :] += np.einsum(\n",
    "                    \"bijo,klco->bklc\",\n",
    "                    dZ[:, row:row+1, col:col+1, :],\n",
    "                    W)\n",
    "        \n",
    "        dX = dX_pad[:, self.pad[0]:self.pad[0] + in_rows, self.pad[1]:self.pad[1] + in_cols, :]\n",
    "\n",
    "        # store the gradients in `self.gradients`\n",
    "        # the gradient for self.parameters[\"W\"] should be stored in\n",
    "        # self.gradients[\"W\"], etc.\n",
    "        self.gradients[\"W\"] = dW\n",
    "        self.gradients[\"b\"] = db\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return dX\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Loss Function Implementations:\n",
    "\n",
    "Implementation of `losses.CrossEntropy`:\n",
    "\n",
    "```python\n",
    "class CrossEntropy(Loss):\n",
    "    \"\"\"Cross entropy loss function.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str) -> None:\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        return self.forward(Y, Y_hat)\n",
    "\n",
    "    def forward(self, Y: np.ndarray, Y_hat: np.ndarray) -> float:\n",
    "        \"\"\"Computes the loss for predictions `Y_hat` given one-hot encoded labels\n",
    "        `Y`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      one-hot encoded labels of shape (batch_size, num_classes)\n",
    "        Y_hat  model predictions in range (0, 1) of shape (batch_size, num_classes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a single float representing the loss\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        a_really_friggin_small_number_safety_first = 5e-324\n",
    "        return - np.sum(Y * np.log(Y_hat + a_really_friggin_small_number_safety_first)) / Y.shape[0]\n",
    "\n",
    "    def backward(self, Y: np.ndarray, Y_hat: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Backward pass of cross-entropy loss.\n",
    "        NOTE: This is correct ONLY when the loss function is SoftMax.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Y      one-hot encoded labels of shape (batch_size, num_classes)\n",
    "        Y_hat  model predictions in range (0, 1) of shape (batch_size, num_classes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the gradient of the cross-entropy loss with respect to the vector of\n",
    "        predictions, `Y_hat`\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        return - Y / (Y_hat * Y.shape[0])\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Model Implementations:\n",
    "\n",
    "Implementation of `models.NeuralNetwork.forward`:\n",
    "\n",
    "```python\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"One forward pass through all the layers of the neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  design matrix whose must match the input shape required by the\n",
    "           first layer\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        forward pass output, matches the shape of the output of the last layer\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Iterate through the network's layers.\n",
    "        Y_hat = X\n",
    "        for layer in self.layers:\n",
    "            Y_hat = layer.forward(Y_hat)\n",
    "        return Y_hat\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `models.NeuralNetwork.backward`:\n",
    "\n",
    "```python\n",
    "    def backward(self, target: np.ndarray, out: np.ndarray) -> float:\n",
    "        \"\"\"One backward pass through all the layers of the neural network.\n",
    "        During this phase we calculate the gradients of the loss with respect to\n",
    "        each of the parameters of the entire neural network. Most of the heavy\n",
    "        lifting is done by the `backward` methods of the layers, so this method\n",
    "        should be relatively simple. Also make sure to compute the loss in this\n",
    "        method and NOT in `self.forward`.\n",
    "\n",
    "        Note: Both input arrays have the same shape.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target  the targets we are trying to fit to (e.g., training labels)\n",
    "        out     the predictions of the model on training data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the loss of the model given the training inputs and targets\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Compute the loss.\n",
    "        L = self.loss(target, out)\n",
    "        # Backpropagate through the network's layers.\n",
    "        dLdY = self.loss.backward(target, out)\n",
    "        for layer in self.layers[::-1]:\n",
    "            dLdY = layer.backward(dLdY)\n",
    "        return L\n",
    "\n",
    "```\n",
    "\n",
    "Implementation of `models.NeuralNetwork.predict`:\n",
    "\n",
    "```python\n",
    "    def predict(self, X: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"Make a forward and backward pass to calculate the predictions and\n",
    "        loss of the neural network on the given data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X  input features\n",
    "        Y  targets (same length as `X`)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a tuple of the prediction and loss\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n",
    "        # Do a forward pass. Maybe use a function you already wrote?\n",
    "        Y_hat = self.forward(X)\n",
    "        # Get the loss. Remember that the `backward` function returns the loss.\n",
    "        L = self.backward(Y, Y_hat)\n",
    "        return (Y_hat, L)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edae737-4844-4687-9408-cdd1f968a78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
